{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from tensorflow.keras.models import load_model,Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,Input,Conv2D,Flatten,MaxPool2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With % train / test / val 70 / 20 / 10 taken from same directory\n",
    "base_dir = '../all_data/'\n",
    "all_norm_files = np.array(glob.glob(base_dir+'NORMAL/*'))\n",
    "all_pnm_files = np.array(glob.glob(base_dir+'PNEUMONIA/*'))\n",
    "np.random.shuffle(all_norm_files)\n",
    "np.random.shuffle(all_pnm_files)\n",
    "\n",
    "def train_test_val(data, ratios, label):\n",
    "    i1 = int(len(data) * ratios['train'])\n",
    "    i2 = int(len(data) * ratios['traintest'])\n",
    "    train = data[:i1]\n",
    "    test = data[i1:i2]\n",
    "    val = data[i2:]\n",
    "    return { \n",
    "        'files': {\n",
    "            'train': train, \n",
    "            'test': test, \n",
    "            'val': val\n",
    "        },\n",
    "        'labels': {\n",
    "            'train': np.array([label]*len(train)),\n",
    "            'test': np.array([label]*len(test)),\n",
    "            'val': np.array([label]*len(val))\n",
    "        }\n",
    "    }\n",
    "\n",
    "data = { 'normal': {}, 'pneumonia':{} }\n",
    "data['normal'] = train_test_val(all_norm_files, {'train': 0.7, 'traintest': 0.9}, 'normal')\n",
    "data['pneumonia'] = train_test_val(all_pnm_files, {'train': 0.7, 'traintest': 0.9}, 'pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../all_data/NORMAL\\\\NORMAL2-IM-0346-0001.jpeg'\n",
      " '../all_data/NORMAL\\\\NORMAL2-IM-0059-0001.jpeg'\n",
      " '../all_data/NORMAL\\\\NORMAL2-IM-1138-0001.jpeg']\n",
      "['../all_data/PNEUMONIA\\\\person1918_bacteria_4825.jpeg'\n",
      " '../all_data/PNEUMONIA\\\\person18_bacteria_57.jpeg'\n",
      " '../all_data/PNEUMONIA\\\\person24_bacteria_109.jpeg']\n",
      "['../all_data/NORMAL\\\\NORMAL2-IM-0918-0001.jpeg'\n",
      " '../all_data/NORMAL\\\\NORMAL2-IM-0207-0001.jpeg'\n",
      " '../all_data/NORMAL\\\\NORMAL2-IM-1203-0001.jpeg']\n",
      "['../all_data/PNEUMONIA\\\\person118_virus_224.jpeg'\n",
      " '../all_data/PNEUMONIA\\\\person257_bacteria_1193.jpeg'\n",
      " '../all_data/PNEUMONIA\\\\person847_bacteria_2767.jpeg']\n",
      "['../all_data/NORMAL\\\\NORMAL2-IM-0775-0001.jpeg'\n",
      " '../all_data/NORMAL\\\\NORMAL2-IM-0385-0001.jpeg'\n",
      " '../all_data/NORMAL\\\\IM-0746-0001.jpeg']\n",
      "['../all_data/PNEUMONIA\\\\person370_bacteria_1687.jpeg'\n",
      " '../all_data/PNEUMONIA\\\\person133_bacteria_634.jpeg'\n",
      " '../all_data/PNEUMONIA\\\\person807_virus_1441.jpeg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [ 21.,  21.,  21.],\n",
       "         [  8.,   8.,   8.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  1.,   1.,   1.],\n",
       "         ...,\n",
       "         [ 18.,  18.,  18.],\n",
       "         [  7.,   7.,   7.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  1.,   1.,   1.],\n",
       "         ...,\n",
       "         [ 16.,  16.,  16.],\n",
       "         [  6.,   6.,   6.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  2.,   2.,   2.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  2.,   2.,   2.],\n",
       "         [  5.,   5.,   5.],\n",
       "         [  9.,   9.,   9.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[126., 126., 126.],\n",
       "         [127., 127., 127.],\n",
       "         [144., 144., 144.],\n",
       "         ...,\n",
       "         [ 84.,  84.,  84.],\n",
       "         [ 56.,  56.,  56.],\n",
       "         [ 50.,  50.,  50.]],\n",
       "\n",
       "        [[118., 118., 118.],\n",
       "         [123., 123., 123.],\n",
       "         [124., 124., 124.],\n",
       "         ...,\n",
       "         [ 71.,  71.,  71.],\n",
       "         [ 58.,  58.,  58.],\n",
       "         [ 46.,  46.,  46.]],\n",
       "\n",
       "        [[111., 111., 111.],\n",
       "         [124., 124., 124.],\n",
       "         [111., 111., 111.],\n",
       "         ...,\n",
       "         [ 60.,  60.,  60.],\n",
       "         [ 57.,  57.,  57.],\n",
       "         [ 48.,  48.,  48.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_size = 160\n",
    "def get_imgs(files):\n",
    "    print(files[:3])\n",
    "    images = np.zeros((len(files), im_size, im_size,3), dtype='float32')\n",
    "    for i, img_file in enumerate(files):\n",
    "        img = np.array(Image.open(img_file).resize((160, 160)).convert(\"RGB\")) \n",
    "        images[i] = img\n",
    "    return images\n",
    "trn_norm_x = get_imgs(data['normal']['files']['train']) # trn_norm_img\n",
    "trn_pnm_x = get_imgs(data['pneumonia']['files']['train'])\n",
    "tst_norm_x = get_imgs(data['normal']['files']['test'])\n",
    "tst_pnm_x = get_imgs(data['pneumonia']['files']['test'])\n",
    "val_norm_x = get_imgs(data['normal']['files']['val'])\n",
    "val_pnm_x = get_imgs(data['pneumonia']['files']['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train normal array shape : (1108, 160, 160, 3)\n",
      "train pneumonia array shape : (2991, 160, 160, 3)\n",
      "\n",
      "test normal array shape : (316, 160, 160, 3)\n",
      "test pneumonia array shape : (854, 160, 160, 3)\n",
      "\n",
      "val normal array shape : (159, 160, 160, 3)\n",
      "val pneumonia array shape : (428, 160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"train normal array shape :\",trn_norm_x.shape)\n",
    "print(\"train pneumonia array shape :\",trn_pnm_x.shape)\n",
    "print(\"\\ntest normal array shape :\",tst_norm_x.shape)\n",
    "print(\"test pneumonia array shape :\",tst_pnm_x.shape)\n",
    "print(\"\\nval normal array shape :\",val_norm_x.shape)\n",
    "print(\"val pneumonia array shape :\",val_pnm_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.append(trn_norm_x, trn_pnm_x, axis=0)\n",
    "y_train = np.append(data['normal']['labels']['train'], data['pneumonia']['labels']['train'])\n",
    "x_test = np.append(tst_norm_x,tst_pnm_x,axis=0)\n",
    "y_test = np.append(data['normal']['labels']['test'], data['pneumonia']['labels']['test'])\n",
    "x_val = np.append(val_norm_x,val_pnm_x,axis=0)\n",
    "y_val = np.append(data['normal']['labels']['val'],data['pneumonia']['labels']['val'])\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False) # NORMAL is [1,0] and PNEUMONIA is [0,1]\n",
    "y_train_enc= encoder.fit_transform(y_train.reshape(-1,1))\n",
    "y_test_enc= encoder.fit_transform(y_test.reshape(-1,1))\n",
    "y_val_enc= encoder.fit_transform(y_val.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rotation_range=45,\n",
    "#                    width_shift_range=0.2,\n",
    "#                    height_shift_range=0.2,\n",
    "#                    shear_range=0.2,\n",
    "#                    zoom_range=0.25,\n",
    "#                    horizontal_flip=True,\n",
    "#                    fill_mode='nearest')\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                   samplewise_center=True,\n",
    "                                   samplewise_std_normalization=True,\n",
    "                                   zoom_range = 0.2, \n",
    "                                   width_shift_range=0.1,  \n",
    "                                   height_shift_range=0.1)\n",
    "\n",
    "train_generator = train_datagen.flow(x_train,\n",
    "                       y_train_enc,\n",
    "                       batch_size=batch_size)\n",
    "\n",
    "\n",
    "# val_datagen = ImageDataGenerator(rotation_range=45,\n",
    "#                    width_shift_range=0.2,\n",
    "#                    height_shift_range=0.2,\n",
    "#                    shear_range=0.2,\n",
    "#                    zoom_range=0.25,\n",
    "#                    horizontal_flip=True,\n",
    "#                    fill_mode='nearest')\n",
    "\n",
    "val_datagen  = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                         samplewise_center=True,\n",
    "                                         samplewise_std_normalization=True,\n",
    "                                         zoom_range = 0.2, \n",
    "                                         width_shift_range=0.1,  \n",
    "                                         height_shift_range=0.1)\n",
    "\n",
    "val_generator = val_datagen.flow(x_val,\n",
    "                       y_val_enc,\n",
    "                       batch_size=batch_size)\n",
    "\n",
    "test_datagen  = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                    samplewise_center=True,\n",
    "                                    samplewise_std_normalization=True)\n",
    "\n",
    "test_generator = test_datagen.flow(x_test,\n",
    "                     y_test_enc,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = train_datagen.flow(x_train,y_train_enc,batch_size=1)\n",
    "# plt.imshow(gen[0][0][0])\n",
    "# print(gen[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "256/256 [==============================] - 194s 748ms/step - loss: 0.3216 - accuracy: 0.8658 - precision: 0.8646 - recall: 0.8665 - val_loss: 0.4699 - val_accuracy: 0.7586 - val_precision: 0.7557 - val_recall: 0.7705\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 178s 693ms/step - loss: 0.2221 - accuracy: 0.9145 - precision: 0.9152 - recall: 0.9145 - val_loss: 0.1525 - val_accuracy: 0.9469 - val_precision: 0.9467 - val_recall: 0.9435\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 120s 470ms/step - loss: 0.2026 - accuracy: 0.9194 - precision: 0.9172 - recall: 0.9199 - val_loss: 0.1353 - val_accuracy: 0.9512 - val_precision: 0.9551 - val_recall: 0.9478\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 199s 776ms/step - loss: 0.1855 - accuracy: 0.9309 - precision: 0.9300 - recall: 0.9309 - val_loss: 0.1285 - val_accuracy: 0.9563 - val_precision: 0.9563 - val_recall: 0.9546\n",
      "Epoch 5/20\n",
      "142/256 [===============>..............] - ETA: 1:34 - loss: 0.1701 - accuracy: 0.9394 - precision: 0.9376 - recall: 0.9380"
     ]
    }
   ],
   "source": [
    "n_train = len(trn_norm_x)+len(trn_pnm_x)\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = tf.keras.Sequential(name='X-ray_CNN')\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(160,160,3)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu')) # relu activation makes each value 0 if it is negative\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=2, activation='sigmoid'))\n",
    "\n",
    "METRICS = ['accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')]\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=METRICS)\n",
    "hist = model.fit(train_generator,\n",
    "           steps_per_epoch= x_train.shape[0] // batch_size,\n",
    "           epochs= epochs,\n",
    "           validation_data= test_generator,\n",
    "           validation_steps= x_test.shape[0] // batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model finished training!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 10s 491ms/step\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# labels = ['bacteria','normal','virus']\n",
    "#confusion matrix\n",
    "y_pred = model.predict(x_test)\n",
    "#transforming label back to original\n",
    "y_pred = encoder.inverse_transform(y_pred).squeeze()\n",
    "#matrix of Actual vs Prediction data\n",
    "# c_matrix = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.title('Confusion matrix',fontsize=14)\n",
    "# sns.heatmap(\n",
    "#   c_matrix, xticklabels=labels,yticklabels=labels,\n",
    "#   fmt='d', annot=True,annot_kws={\"size\": 14}, cmap='Reds')\n",
    "# plt.xlabel(\"Predicted\",fontsize=12)\n",
    "# plt.ylabel(\"Actual\",fontsize=12)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711340206185567"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.squeeze() == y_test.squeeze()).sum() / len(y_test) # Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 363 64 11\n",
      "{'Accuracy': 0.8711340206185567, 'Precision': 0.6923076923076923, 'Recall': 0.9290322580645162, 'F1 Score': 0.7933884297520661}\n"
     ]
    }
   ],
   "source": [
    "# len(y_pred), len(y_test), np.array(y_pred.squeeze() == y_test.squeeze(), dtype=np.int64).sum()\n",
    "# y_test.squeeze()\n",
    "# y_pred_np = y_pred.squeeze()\n",
    "y_pred = y_pred.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "# Make sure both input arrays have the same length\n",
    "if len(y_pred) != len(y_test):\n",
    "    raise ValueError(\"Input arrays must have the same length\")\n",
    "\n",
    "# True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN)\n",
    "TP = np.sum(np.logical_and(y_pred == 'normal', y_test == 'normal'))\n",
    "TN = np.sum(np.logical_and(y_pred == 'pneumonia', y_test == 'pneumonia'))\n",
    "FP = np.sum(np.logical_and(y_pred == 'normal', y_test == 'pneumonia'))\n",
    "FN = np.sum(np.logical_and(y_pred == 'pneumonia', y_test == 'normal'))\n",
    "print(TP, TN, FP, FN)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "# Precision\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "\n",
    "# Recall (Sensitivity or True Positive Rate)\n",
    "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "\n",
    "# F1 Score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "print({\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1_score\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pneumonia    427\n",
       "normal       155\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: xraymodel_tertiary_20231016_123547\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: xraymodel_tertiary_20231016_123547\\assets\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "model.save('xraymodel_tertiary_'+str(datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "19/19 [==============================] - 7s 359ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     plt\u001b[39m.\u001b[39mtitle(title, fontsize\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m---> 26\u001b[0m plot_confusion(\u001b[39mlen\u001b[39;49m(x_test), test_generator, \u001b[39m'\u001b[39;49m\u001b[39mTest set\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size)\n",
      "Cell \u001b[1;32mIn [13], line 18\u001b[0m, in \u001b[0;36mplot_confusion\u001b[1;34m(n, generator, title, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# print(preds, n)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(preds, n)\n\u001b[1;32m---> 18\u001b[0m cm  \u001b[39m=\u001b[39m confusion_matrix(labels, preds[:\u001b[39mlen\u001b[39;49m(labels)])\n\u001b[0;32m     19\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m     20\u001b[0m plot_confusion_matrix(cm,figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m8\u001b[39m), hide_ticks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,cmap\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39mcm\u001b[39m.\u001b[39mBlues)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[0;32m    223\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    224\u001b[0m ):\n\u001b[0;32m    225\u001b[0m     \u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    309\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    }
   ],
   "source": [
    "def plot_confusion(n, generator, title, batch_size):\n",
    "    labels = []\n",
    "    for i in range(0, n//batch_size):\n",
    "        # print(i)\n",
    "        labels.extend(generator[i][1])\n",
    "    labels = np.array(labels)\n",
    "    print(len(labels))\n",
    "    # preds = model.predict_classes(generator)\n",
    "    preds = (model.predict(generator) > 0.5).astype(\"int32\")[:,0]\n",
    "    # print(preds, n)\n",
    "    preds = np.reshape(preds, n)\n",
    "\n",
    "    cm  = confusion_matrix(labels, preds[:len(labels)])\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=False,cmap=plt.cm.Blues)\n",
    "    plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "    plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "    plt.title(title, fontsize=20)\n",
    "    return plt.show()\n",
    "\n",
    "plot_confusion(len(x_test), test_generator, 'Test set', batch_size)\n",
    "# plot_confusion(n_train, train_generator, 'Training set', 16)\n",
    "# plot_confusion(n_validation, validation_generator, 'Validation set', 16)\n",
    "\n",
    "# 0.973958 accuracy for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4696, 4696)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc7e758b011b8546c2d8d6136d2d13627abe8c022b73c6b30824a33af506f0a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
